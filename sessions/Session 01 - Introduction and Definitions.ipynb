{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029803d0",
   "metadata": {},
   "source": [
    "## Basic definitions\n",
    "\n",
    "### Neurons\n",
    "\n",
    "In a truest sense of the word, saying the following equation is the definition of a neuron is oversimplification in term of biology, however, let's just accept it and move forward.\n",
    "\n",
    "A neuron is a function that given input x, will calculate the output y with activation function f:\n",
    "$ y = f(w*x + b) $, in which:\n",
    "\n",
    "- y is the output\n",
    "- x is the input\n",
    "- w is the weight of the neuron\n",
    "- b is the bias\n",
    "- f is the activation function\n",
    "\n",
    "If $f$ is an identity function, then this becomes a linear function. Hence, you can see that activation function plays a crucial role in making our neural network a non-linear model, which mathematically speaking, can represent a lot more thing compare to a simple linear model.\n",
    "\n",
    "The trainable part of a neuron is normally the weight and bias. Sometimes, there can be trainable activation functions.\n",
    "\n",
    "![a neuron](https://nickmccullum.com/images/python-deep-learning/understanding-neurons-deep-learning/neuron-functionality.png)\n",
    "\n",
    "### Neural network\n",
    "\n",
    "A neural network is just a network of neurons (duh).\n",
    "The most basic of neural network is a dense network, with every neurons in the previous layer connected to every neurons in the next layer.\n",
    "\n",
    "![a neural network](https://miro.medium.com/max/578/1*ToPT8jnb5mtnikmiB42hpQ.png)\n",
    "\n",
    "However, if you do the math, this type of dense connection can make a neural network very huge with just a few layers. There are other types of neuron configuration that can be much more efficient, one of the most used for image and video is convolution. But this will be explored in another session.\n",
    "\n",
    "### Common activation functions\n",
    "\n",
    "We see above that activation functions help us create non-linear models. So, what are the common activation functions?\n",
    "\n",
    "#### ReLU and it's family\n",
    "\n",
    "ReLU is one of the simplest but also most widely used family of activation function. It stands for rectified linear unit.\n",
    "\n",
    "The original ReLU function: \n",
    "$$\n",
    "f(x)= \n",
    "\\begin{cases}\n",
    "x & \\text{if} & x>0\\\\\n",
    "0 & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The original ReLU has a dying ReLU problem where there is no gradient backflow due to negative values are all being squashed into a big fat 0.\n",
    "\n",
    "There are a lot of other types of ReLU, such as leaky ReLU, ELU, etc.\n",
    "\n",
    "#### Sigmoid\n",
    "$$\n",
    "f(x) = {1\\over{1+e^{-x}}}\n",
    "$$\n",
    "Returned value of sigmoid has a range of 0 to 1.\n",
    "\n",
    "#### Tanh\n",
    "$$\n",
    "f(x) = {{1-e^{-2x}}\\over{1+e^{-2x}}}\n",
    "$$\n",
    "Returned value of tanh has a range of -1 to 1.\n",
    "\n",
    "#### Softmax\n",
    "Softmax is a special function, it receives a list of inputs and normalizes them into a probability distribution (sum of all outputs is 1), hence it is mostly used as the last activation function for classification model.\n",
    "$$\n",
    "f(\\vec{x})_{i} = {{e^{x_i}}\\over{\\sum_{j=1}^{K}{e^{x_j}}}}\n",
    "$$\n",
    "$K$ is the size of the input vector $\\vec{x}$.\n",
    "\n",
    "### Now, let's \"handcraft\" a neural network model!\n",
    "\n",
    "Consider a 1 dimensional world with 2 countries A and B, and there border is at point x = 3. It means that all of the land with value < 3 belongs to country  A and all of the land with value >= 3 belongs to country B. Now create a model that represent this!\n",
    "\n",
    "First, we can convert the 2 classes into a number representative:\n",
    "\n",
    "A: 0\n",
    "\n",
    "B: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2bac831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: float) -> float:\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x > 0:\n",
    "        return x\n",
    "    \n",
    "weight = 1\n",
    "bias = 1\n",
    "dumb_net = lambda x: relu(weight * x + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42c51534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Test, expect everything to be True\n",
    "print([\n",
    "    dumb_net(2.9) == 0,\n",
    "    dumb_net(-1) == 0,\n",
    "    dumb_net(3) > 0,\n",
    "    dumb_net(100000) > 0\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdede80",
   "metadata": {},
   "source": [
    "Let's us \"train\" ourselves and optimize the model by changing the value of the weight and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e39a41",
   "metadata": {},
   "source": [
    "## To train a neural network\n",
    "\n",
    "We have managed to created a very simple model, but currently we have no systematic way to iteratively optimize it.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "First of all, to optimize anything, we need to have a way of calculate how far we are from the target.\n",
    "\n",
    "For the following equation, we will assume:\n",
    "- $y$ stands for the desired (truth) value\n",
    "- $\\hat{y}$ stands for the predicted value\n",
    "- $L$ stands for the loss function\n",
    "\n",
    "Some of the common loss functions:\n",
    "\n",
    "#### Mean squared error loss (MSE) - for regression problem\n",
    "One of the standard and most popular loss function for real value prediction (continuous value prediction).\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n}({y_i - \\hat{y}_i})^2\n",
    "$$\n",
    "\n",
    "There are other flavors of MSE, such as mean squared logarithmic error loss or mean absolute error loss.\n",
    "\n",
    "#### Cross-entropy loss - for classification problem\n",
    "\n",
    "If there are just 2 classes, we can use binary cross-entropy loss, but noted that the final layer activation should be a sigmoid activation.\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\frac{1}{N}\\sum_{i=1}^{N}(y_i*log(\\hat{y}_i) + (1-y_i)*log(1-\\hat{y}_i))\n",
    "$$\n",
    "\n",
    "Multi-class cross entropy loss is used for multi-class classification. The truth matrix should be one hot encoded, meaning each class is totally independent from other classes. One-hot encoded matrix has the shape of (number of output, number of classes). If the output belong to a class, then only the corresponding class has value of 1, the rest is 0.\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\sum_{k}^{K}y_k*\\hat{y}_k\n",
    "$$\n",
    "\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "Chain rule is fairly simple:\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}\n",
    "$$\n",
    "$\\frac{dy}{dx}$: derivative of y with respect to x\n",
    "\n",
    "$\\frac{dy}{du}$: derivative of y with respect to u\n",
    "\n",
    "$\\frac{dy}{dx}$: derivative of u with respect to x\n",
    "\n",
    "But this is the key to make a neural network trainable. \n",
    "\n",
    "As you may notice, a neural network is just a big equation, and with chain rule, we can break it up into smaller parts, that in turn we can calculate how changing a smaller part can affect the final prediction.\n",
    "\n",
    "You might also notice that since differentiation is one of the key in training a neural network, hence, all of the components of the neural network should be differentiable, from the neurons, to the activation functions to the loss functions. (ReLU is not exactly differentiable, but it is differentiable at all point except 0).\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "Backpropagation is a method to update the weights of the neural network by taking into account the actual output and the desired output. The derivative with respect to each weight is computed using the chain rule.\n",
    "\n",
    "Let's denote:\n",
    "- $\\partial$: partial derivative\n",
    "- $f$: the entire neural network model\n",
    "- $L$: the loss function\n",
    "\n",
    "Applying chain rule, we will have:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{f(x)}}*\\frac{\\partial{f(x)}}{\\partial{x}}\n",
    "$$\n",
    "Meaning that changes to the loss function with respect to the input equal to the changes of loss function with respect to the neural network time the changes of the neural network with respect to the input.\n",
    "\n",
    "Moreover, we can express the loss function in term of the input and the neural network $f$:\n",
    "$$\n",
    "L(y, \\hat{y}) = L(y, f(x))\n",
    "$$\n",
    "\n",
    "To update (train) the network, we just have to go against the loss gradient (in order to minimize the loss).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633fd2f",
   "metadata": {},
   "source": [
    "### Real example\n",
    "\n",
    "Consider our `dumb_net` above, the entire network can be represented by this equation:\n",
    "$$\n",
    "f(x) = reLU(w*x + b) = reLU(W(x))\n",
    "$$\n",
    "with $W(x)$ represents the neuron function.\n",
    "\n",
    "Let's use MSE loss (even though this is actually a classification problem, but let's ignore it for now).\n",
    "Then we will have the following equation representing the loss:\n",
    "$$\n",
    "L(y, \\hat{y}) = L(y, f(x)) = \\frac{1}{n}\\sum_{i=1}^{n}({y_i - f(x_i)})^2\n",
    "$$\n",
    "$$\n",
    "L(y, (fx)) = \\frac{1}{n}\\sum_{i=1}^{n}({y_i - reLU(w*x + b)})^2\n",
    "$$\n",
    "\n",
    "Now, in order to optimize/update/train this network, we need to minimize the above loss function.\n",
    "\n",
    "The derivative of MSE loss function with respect to our network:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{f(x)}} = f(x) - y\n",
    "$$\n",
    "The derivative of reLU activation function with respect to x:\n",
    "$$\n",
    "\\frac{\\partial{reLU}}{\\partial{W(x)}}= \n",
    "\\begin{cases}\n",
    "1 & \\text{if} & W(x)>0\\\\\n",
    "0 & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "Working backwards, we have:\n",
    "$$\n",
    "\\frac{\\partial{W}}{\\partial{w}}= x \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{W}}{\\partial{b}}= 1\n",
    "$$\n",
    "with $w$ is the weight and $b$ is the bias. We want to actually update the weight and bias, remember?\n",
    "\n",
    "Chain them up, we will have:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{w}} = \\frac{\\partial{L}}{\\partial{f}}\\frac{\\partial{f}}{\\partial{W}}\\frac{\\partial{W}}{\\partial{w}}\n",
    "$$\n",
    "Bias will have a similar equation like above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82565140",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "With chain rule, we got a nice equation to minimize, but how do we actually minimize it? What should be the strategy?\n",
    "The most simple method is stochastic gradient descent:\n",
    "$$\n",
    "w = w - n * \\frac{\\partial{L}}{\\partial{w}}\n",
    "$$\n",
    "where $n$ is the learning rate or the step size.\n",
    "\n",
    "There are other better optimizing strategies, such as Adam and its family. I won't go into details for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83152d2a",
   "metadata": {},
   "source": [
    "If we have time, we can also train our `dumb_net` manually using stochastic gradient descent, but it's been a long time and my calculus is getting a bit rusty, so I will let you do it on your own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975182c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
