{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4595f76",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There are a lot of different machine learning framework, ranging from more traditional ML methods like random forest, linear regression etc. (scikit-learn) to deep learning and neural network focused ([pyTorch](https://pytorch.org/), [Tensorflow](https://www.tensorflow.org/), [Darknet](https://pjreddie.com/darknet/), etc.). Then we have the framework that provide more than just the ML methods like HuggingFace, SparkML, MLFlow, etc. \n",
    "\n",
    "I know we are interested in deep learning and neural network here, so you will at least need to know the name of pyTorch (and Torch), Tensorflow and Keras. PyTorch was developed by Facebook and Tensorflow was developed by Google. Keras was the new SDK/interface on top of Tensorflow that make it easier to use, it used to be a standalone library but now it is integrated into Tensorflow.\n",
    "\n",
    "PyTorch is famous for its ease of use (and its dynamic graph) while Tensorflow is more production ready. However, that was the early days, currently, they are quite similar, even in term of syntax.\n",
    "\n",
    "The differences between ML frameworks shrink even further with the [Open Neural Network Exchange (ONNX)](https://onnx.ai/), ONNX aims to be the common format for neural network model, so you can develop your model in any of the existing framework and can easily import/export the model from/to ONNX format.\n",
    "\n",
    "For our session, we will look at pyTorch since it is still slightly easier to use than Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44e697",
   "metadata": {},
   "source": [
    "## PyTorch 101\n",
    "\n",
    "So from the previous session, what are the most common components of a neural network (and the training of such network)?\n",
    "They are:\n",
    "- The dataset (input and ground truth)\n",
    "- The model (with different type of layers and activation functions)\n",
    "- The loss function\n",
    "- The optimizer\n",
    "\n",
    "### The dataset\n",
    "PyTorch provide a class to define a `Dataset`:\n",
    "```python\n",
    "from torch.utils.data.dataset import Dataset\n",
    "```\n",
    "\n",
    "To create your own dataset, you can extend from this and implement  the `__len__` and `__getitem__` method. Think if this as a Python generator.\n",
    "\n",
    "A more elabored example:\n",
    "```python\n",
    "from typing import Tuple\n",
    "from abc import abstractmethod\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class IImageClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Interface for image classification dataset.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        :param index: sample position inside dataset\n",
    "        :return: tuple of input image and groundtruth class as tensors\n",
    "            Input image shape: [C, H, W]\n",
    "                C: Number of channels for input image. 1 for intensity-based,\n",
    "                3 for color-based.\n",
    "            Output label shape: [N]\n",
    "                N: Number of classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "```\n",
    "\n",
    "However, it is not a must to provide a Dataset as input, we can also simply convert our inputs into tensors and feed them to our network.\n",
    "\n",
    "### The model\n",
    "\n",
    "The most basic object of a neural network is the `Tensor`. They are the data that is being passed around, from the inputs, to the weights and biases, to the outputs. They are all `Tensor`.\n",
    "```python\n",
    "from torch import Tensor\n",
    "```\n",
    "\n",
    "The neural network layers reside in `torch.nn` module. Some basic neural network layers:\n",
    "```python\n",
    "from torch import nn\n",
    "nn.Linear # apply a linear transformation to the input\n",
    "nn.Conv1d # apply a convolution to the input, 1 dimensional\n",
    "nn.Conv2d\n",
    "nn.Conv3d\n",
    "nn.RNN    # apply a recurrent layer\n",
    "nn.LSTM\n",
    "nn.GRU\n",
    "nn.MaxPool1d # Max pooling layer, 1 dimensional\n",
    "nn.Dropout   # Dropout layer, a powerful yet simple regularization technique\n",
    "...\n",
    "\n",
    "# Activation\n",
    "nn.ReLU\n",
    "nn.LeakyReLU\n",
    "nn.Softmax\n",
    "...\n",
    "```\n",
    "\n",
    "A neural network model is defined by extending the `nn.Module` class. We will need to define at least 2 methods:\n",
    "- `__init__`: define what are the layers in your network\n",
    "- `forward`: define how the data is being passed through your network\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876ead06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.max_pool2d = nn.MaxPool2d((2))\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = self.max_pool2d(x)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = self.log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bfd63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3570, -2.3486, -2.1649, -2.4237, -2.3033, -2.2226, -2.4016, -2.2909,\n",
      "         -2.2010, -2.3461]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Equates to one random 28x28 image\n",
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "my_nn = Net()\n",
    "result = my_nn(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e21f77",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Loss functions also reside in `torch.nn` module.\n",
    "Refer to [this](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Example:\n",
    "```python\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### The optimizer\n",
    "\n",
    "Optimizers reside in `torch.optim` module. Refer to [this](https://pytorch.org/docs/stable/optim.html#).\n",
    "\n",
    "Example:\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e747a46",
   "metadata": {},
   "source": [
    "## Hands-on example\n",
    "\n",
    "Consider the same simple example from Session 1.\n",
    "\n",
    "Consider a 1 dimensional world with 2 countries A and B, and there border is at point x = 3. It means that all of the land with value < 3 belongs to country  A and all of the land with value >= 3 belongs to country B. Now create a model that represent this!\n",
    "\n",
    "First, we can convert the 2 classes into a number representative:\n",
    "\n",
    "A: 0\n",
    "\n",
    "B: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7d87c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        self.dataset = [\n",
    "            (Tensor([[2.99]]), Tensor([[0, 1]])),\n",
    "            (Tensor([[-1]]), Tensor([[0, 1]])),\n",
    "            (Tensor([[3]]), Tensor([[1, 0]])),\n",
    "            (Tensor([[3.000001]]), Tensor([[1, 0]])),\n",
    "            (Tensor([[2.99999]]), Tensor([[0, 1]])),\n",
    "            (Tensor([[3.000001]]), Tensor([[1, 0]])),\n",
    "            (Tensor([[10]]), Tensor([[1, 0]])),\n",
    "        ]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, Tensor]:\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b931c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.9900]]), tensor([[0., 1.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SimpleDataset()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "53888709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self._first_layer = nn.Linear(1, 2, bias=False)\n",
    "        self._log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        first = self._first_layer(x)\n",
    "        \n",
    "        output = self._log_softmax(first)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a6c7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48237f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] loss: 2.728\n",
      "[Epoch: 2] loss: 2.700\n",
      "[Epoch: 3] loss: 2.672\n",
      "[Epoch: 4] loss: 2.644\n",
      "[Epoch: 5] loss: 2.616\n",
      "[Epoch: 6] loss: 2.588\n",
      "[Epoch: 7] loss: 2.561\n",
      "[Epoch: 8] loss: 2.533\n",
      "[Epoch: 9] loss: 2.506\n",
      "[Epoch: 10] loss: 2.479\n",
      "[Epoch: 11] loss: 2.452\n",
      "[Epoch: 12] loss: 2.426\n",
      "[Epoch: 13] loss: 2.399\n",
      "[Epoch: 14] loss: 2.373\n",
      "[Epoch: 15] loss: 2.347\n",
      "[Epoch: 16] loss: 2.321\n",
      "[Epoch: 17] loss: 2.295\n",
      "[Epoch: 18] loss: 2.269\n",
      "[Epoch: 19] loss: 2.243\n",
      "[Epoch: 20] loss: 2.218\n",
      "[Epoch: 21] loss: 2.192\n",
      "[Epoch: 22] loss: 2.167\n",
      "[Epoch: 23] loss: 2.142\n",
      "[Epoch: 24] loss: 2.117\n",
      "[Epoch: 25] loss: 2.092\n",
      "[Epoch: 26] loss: 2.067\n",
      "[Epoch: 27] loss: 2.043\n",
      "[Epoch: 28] loss: 2.018\n",
      "[Epoch: 29] loss: 1.994\n",
      "[Epoch: 30] loss: 1.970\n",
      "[Epoch: 31] loss: 1.946\n",
      "[Epoch: 32] loss: 1.923\n",
      "[Epoch: 33] loss: 1.899\n",
      "[Epoch: 34] loss: 1.876\n",
      "[Epoch: 35] loss: 1.853\n",
      "[Epoch: 36] loss: 1.830\n",
      "[Epoch: 37] loss: 1.807\n",
      "[Epoch: 38] loss: 1.784\n",
      "[Epoch: 39] loss: 1.762\n",
      "[Epoch: 40] loss: 1.739\n",
      "[Epoch: 41] loss: 1.717\n",
      "[Epoch: 42] loss: 1.695\n",
      "[Epoch: 43] loss: 1.674\n",
      "[Epoch: 44] loss: 1.652\n",
      "[Epoch: 45] loss: 1.631\n",
      "[Epoch: 46] loss: 1.610\n",
      "[Epoch: 47] loss: 1.589\n",
      "[Epoch: 48] loss: 1.568\n",
      "[Epoch: 49] loss: 1.548\n",
      "[Epoch: 50] loss: 1.528\n",
      "[Epoch: 51] loss: 1.507\n",
      "[Epoch: 52] loss: 1.488\n",
      "[Epoch: 53] loss: 1.468\n",
      "[Epoch: 54] loss: 1.449\n",
      "[Epoch: 55] loss: 1.429\n",
      "[Epoch: 56] loss: 1.410\n",
      "[Epoch: 57] loss: 1.392\n",
      "[Epoch: 58] loss: 1.373\n",
      "[Epoch: 59] loss: 1.355\n",
      "[Epoch: 60] loss: 1.337\n",
      "[Epoch: 61] loss: 1.319\n",
      "[Epoch: 62] loss: 1.301\n",
      "[Epoch: 63] loss: 1.284\n",
      "[Epoch: 64] loss: 1.267\n",
      "[Epoch: 65] loss: 1.250\n",
      "[Epoch: 66] loss: 1.233\n",
      "[Epoch: 67] loss: 1.217\n",
      "[Epoch: 68] loss: 1.201\n",
      "[Epoch: 69] loss: 1.185\n",
      "[Epoch: 70] loss: 1.169\n",
      "[Epoch: 71] loss: 1.154\n",
      "[Epoch: 72] loss: 1.139\n",
      "[Epoch: 73] loss: 1.124\n",
      "[Epoch: 74] loss: 1.109\n",
      "[Epoch: 75] loss: 1.095\n",
      "[Epoch: 76] loss: 1.081\n",
      "[Epoch: 77] loss: 1.067\n",
      "[Epoch: 78] loss: 1.054\n",
      "[Epoch: 79] loss: 1.040\n",
      "[Epoch: 80] loss: 1.027\n",
      "[Epoch: 81] loss: 1.015\n",
      "[Epoch: 82] loss: 1.002\n",
      "[Epoch: 83] loss: 0.990\n",
      "[Epoch: 84] loss: 0.978\n",
      "[Epoch: 85] loss: 0.966\n",
      "[Epoch: 86] loss: 0.955\n",
      "[Epoch: 87] loss: 0.944\n",
      "[Epoch: 88] loss: 0.933\n",
      "[Epoch: 89] loss: 0.922\n",
      "[Epoch: 90] loss: 0.912\n",
      "[Epoch: 91] loss: 0.902\n",
      "[Epoch: 92] loss: 0.892\n",
      "[Epoch: 93] loss: 0.882\n",
      "[Epoch: 94] loss: 0.873\n",
      "[Epoch: 95] loss: 0.864\n",
      "[Epoch: 96] loss: 0.855\n",
      "[Epoch: 97] loss: 0.847\n",
      "[Epoch: 98] loss: 0.838\n",
      "[Epoch: 99] loss: 0.830\n",
      "[Epoch: 100] loss: 0.822\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataset, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        if i % 10 == 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'[Epoch: {epoch + 1}] loss: {running_loss / len(dataset):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d43442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4222, 0.5778]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(model(Tensor([[3.1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32aedd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3267],\n",
       "         [-0.2255]], requires_grad=True)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff983b7",
   "metadata": {},
   "source": [
    "Given the current training samples, this might be the best (or at least the local minimum) fit. This is an example of neural network being an approximation and not an analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1abfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
